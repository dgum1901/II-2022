---
output:
  pdf_document: 
    toc_depth: 0
    includes:
      in_header: preambulo.tex
      before_body: titulo.tex
documentclass: memoir
classoption: oneside
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
require("knitcitations")
require("bibtex")
library(rmdformats)
library(readxl)
library(dplyr)
library(plyr)
library(fOptions)
library(ggplot2)
library(kableExtra)
library(extrafont)
library(scales)
library(xtable)
library(corrplot)
options(max.print="75")
opts_chunk$set(echo=FALSE, 
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)

```

\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}
\lhead{CA-409 Distribución de Pérdidas}
\rhead{Bitácora \thechapter}
\fancyfoot[C]{\thepage}

\tableofcontents*

\newpage
\chapter{Bitácora I}


\section{Sobre la base de datos}

```{r}
base <- read.csv('~/GitHub/II-2022/Distribución de pérdidas/Fire Incidents Data.csv')

perd <- base$Estimated_Dollar_Loss
#Los elementos de la base donde no se cuantificaron perdidas no interesan
base <- base[!is.na(perd),]
perd <- perd[!is.na(perd)]


```


```{r}
p <- read.csv('~/GitHub/II-2022/Distribución de pérdidas/columnas.txt',sep = ',', fileEncoding = "UTF-8")

knitr::kable(p, caption = "Descripción completa de la base", longtable = TRUE) %>%
  column_spec(2, width = "7cm") %>% footnote(general = 'Fuente: Elaboración propia', general_title = '') %>%
  kable_styling(latex_options = c("repeat_header"),
                repeat_header_continued = "Continua en siguiente pag",
                repeat_header_text = "\\textit{(Continuación)}"
                )

```

\chapter{Bitácora II}

Dado que la pregunta de investigación se centra en encontrar las bases que generan que un siniestro de tipo incendio sea de mayor cuantía en cuanto a pérdidas, por lo que este primer cuadro analiza la correlacion entre el log-estimado de perdidas (podria ser perfectamente el estimado regular) y las variables numéricas, para ello se utiliza select, y se genera una nueva variable numérica que corresponde a la diferencia entre el tiempo de notificación y el tiempo de llegada de las autoridades al lugar del siniestro.

En cuanto a la metodología para obtener la correlación, y pensando más que todo en que los datos podrían no necesariamente obedecer a modelos estríctamente lineales, se utilizó 

```{r}
base$logEstimado <- log(base$Estimated_Dollar_Loss)
base$logEstimado <- case_when(base$Estimated_Dollar_Loss == 0 ~ 0,
                              TRUE ~ log(base$Estimated_Dollar_Loss))

base$TFS_Alarm_Time <- as.POSIXct(base$TFS_Alarm_Time, format = "%Y-%m-%dT%H:%M:%S")
base$TFS_Arrival_Time <- as.POSIXct(base$TFS_Arrival_Time, format = "%Y-%m-%dT%H:%M:%S")

base$tiempoAtencion <- as.numeric(difftime(base$TFS_Arrival_Time, base$TFS_Alarm_Time))


# El pairwise.complete.obs es para garantizar que tome todas las observaciones
# completas por pares, para más consultas favor consultarme
p <- base %>% dplyr::select(is.numeric & !Estimated_Dollar_Loss) %>% cor(use = "pairwise.complete.obs", method = "spearman")


#xtable(tibble(colnames(p)))
colnames(p) <- 1:14
rownames(p) <- 1:14
xtable(p)

#corrplot(p)

```

```{r}
# #Este código exporta el gráfico en formato PDF
# pdf("corrPlor.pdf")
# corrplot(p)
# dev.off()

```


Para iniciar hablando de la distribución del estimado de pérdidas.

```{r}
# p <- tibble(summary(base$logEstimado), summary(base$Estimated_Dollar_Loss))
# p <- rbind(p, c(sd(base$logEstimado), sd(base$Estimated_Dollar_Loss)))
# p <- rbind(p, c(IQR(base$logEstimado), IQR(base$Estimated_Dollar_Loss)))
# colnames(p) <- c('Log pérdida estimada', 'Pérdida estimada')
# rownames(p) <- c('Mínimo', '1er cuartil','Mediana','Promedio','Tercer cuartil','Máximo', 'Desviación', 'IQr')
# IQR(base$logEstimado)
# xtable(p)
```


```{r}
# p <-
#   aggregate(
#     base$Estimated_Dollar_Loss,
#     by = list(Category = base$Smoke_Alarm_at_Fire_Origin),
#     FUN = sum
#   )
# p$Frecuencia <- count(base$Smoke_Alarm_at_Fire_Origin)$freq
# p$Media <- p$x/p$Frecuencia
# 
# 
# #p <- p[p$x > out,]
# p <- p[order(-p$Media),]
# xtable(p)
```


```{r}

# p <-
#   aggregate(
#     base$Estimated_Dollar_Loss,
#     by = list(Category = base$Property_Use),
#     FUN = sum
#   )
# p$Frecuencia <- count(base$Property_Use)$freq
# p$Media <- p$x/p$Frecuencia
# 
# #p <- p[p$x > out,]
# 
# a <- p[order(-p$x),]
# 
# a <- a[1:10,]
# 
# xtable(a)
# 
# a <- p[order(-p$Media),]
# 
# a <- a[1:10,]
# 
# xtable(a)
# 
# 

```





\section{Graficos}

```{r}

out <- quantile(perd, 0.95) #cuantil del 95 por ciento
grafico0 <-
  ggplot(base, aes(x = Estimated_Dollar_Loss)) + 
  geom_histogram(color =
                   "darkblue", fill = "lightblue") + theme_bw() +
  labs(
    y = "Frecuencia",
    x = "Pérdida en escala regular",
    title = "Gráfico 1: Histograma en escala regular de las pérdidas",
    caption = "Fuente: Elaboración propia"
  )

grafico0


grafico1 <-
  ggplot(base, aes(x = log(Estimated_Dollar_Loss))) + 
  geom_histogram(color =
                   "darkblue", fill = "lightblue") + theme_bw() +
  labs(
    y = "Frecuencia",
    x = "Pérdida en escala logarítmica",
    title = "Gráfico 1: Histograma en escala logarítmica de las pérdidas",
    caption = "Fuente: Elaboración propia"
  )

grafico1

base$Extremo <- perd >= out

grafico2 <-
  ggplot(data = base) + geom_bar(
    mapping = aes(x = Extremo),
    color = c('blue', 'red'),
    fill = c('lightblue', 'pink'),
    width = 0.5,
    na.rm = TRUE
  ) + theme_bw() +
  labs(
    y = "Frecuencia",
    x = "Cuantil",
    title = "Gráfico 1: Frecuencia del estimado de pérdidas de acuerdo al cuantil 0.95",
    caption = "Fuente: Elaboración propia"
  ) +
  scale_x_discrete(labels = c("Menor 95%", "Mayor o igual 95%"))
grafico2


p <-
  data.frame(
    Name = c("Masa cuantil menor a .95", "Masa cuantil mayor o igual a .95"),
    Amount = c(sum(perd[perd < out]), sum(perd[perd >= out]))
  )

grafico3 <- ggplot(p, aes(x = Name, y = Amount)) + geom_bar(
  stat = "identity",
  color = c('blue', 'red'),
  fill = c('lightblue', 'pink'),
  width = 0.5,
  na.rm = TRUE
) + theme_bw() +
  labs(
    y = "Masa de la pérdida",
    x = "Cuantil",
    title = "Gráfico 3: Masa del estimado de pérdidas de acuerdo al cuantil 0.95",
    caption = "Fuente: Elaboración propia"
  ) + scale_y_continuous(labels = comma) + scale_x_discrete(limits = p$Name)
grafico3

```


```{r}
# pdf("graf0.pdf")
# grafico0
# dev.off()
```



\section{Bitacora 3}

Dado que se utiliza el ajuste con la función fevd, estos son los únicos resultados


```{r}
p <- base$logEstimado[base$Extremo]

#c("GEV", "GP", "PP", "Gumbel", "Exponential")
library(extRemes)
gev <- fevd(p,type="GEV", method = 'MLE') #Generalized Extreme Value
s <- summary(gev)
( gev.comp <- c(s$AIC[[1]],s$BIC[[1]], -s$nllh) )

gp <- fevd(p,type="GP", threshold = log(out), method = 'MLE') #Generalized Pareto
s <- summary(gp)
( gp.comp <- c(s$AIC[[1]],s$BIC[[1]], -s$nllh) )

gumbel <- fevd(p,type="Gumbel", method = 'MLE') #Gumbel
s <- summary(gumbel)
( gumbel.comp <- c(s$AIC[[1]],s$BIC[[1]], -s$nllh) )

pp <- fevd(p,type="PP", threshold = log(out), method = 'MLE') #Poisson Process
s <- summary(pp)
( pp.comp <- c(s$AIC[[1]],s$BIC[[1]], s$nllh) )

exponencial <- fevd(p,type="Exponential", threshold = log(out), method = 'MLE') #exponencial
s <- summary(exponencial)
( exponencial.comp <- c(s$AIC[[1]],s$BIC[[1]], -s$nllh) )

df <-
  data.frame(
    GEV = gev.comp,
    GP = gp.comp,
    Gumbel = gumbel.comp,
    PP = pp.comp,
    Exponencial = exponencial.comp
  )
rownames(df) <- c('AIC','BIC', 'Log-verosimilitud')
#xtable(df)
df

```

Ahora se busca ajustar Weibull, Frechet y Beta

```{r}


library(fitdistrplus)
weibull <- fitdist(p, "weibull")
weibull.comp <- c(weibull$aic, weibull$bic, weibull$loglik)

library(extraDistr)
frechet<-fitdist(p,"frechet", start = list( lambda = 1, mu = 1, sigma = 1))
frechet.comp <- c(frechet$aic, frechet$bic, frechet$loglik)

# dbeta_d <- function(x, alpha = -1, theta = 1){
#   #alpha * (-x)^(-alpha-1)
#   -alpha / theta * (-(x-theta)/theta)^(-alpha-1)
# }
beta <- fitdist(p, 'betapr', start = list(  shape1=1, shape2=1))

# beta <- fitdist(p, dbeta_d, start = list( alpha = 3000 , theta = 300))
beta.comp <- c(beta$aic, beta$bic, beta$loglik)

df <-
  data.frame(
    Weibull = weibull.comp,
    Frechet = frechet.comp,
    Beta = beta.comp
  )
rownames(df) <- c('AIC','BIC', 'Log-verosimilitud')
xtable(df)
df

```
Reajustamos las primeras funciones con fitdist para hacer más facil la graficacion
```{r}

gev2 <- fitdist(p, "gev", start = list(mu = 0, sigma = 1, xi = 2.302585 ))
gev2.comp <- c(gev2$aic, gev2$bic, gev2$loglik)

library(actuar)
gp2 <- fitdist(p, 'genpareto' , start=list(shape1 = 1, shape2=1))

gp2.comp <- c(gp2$aic, gp2$bic, gp2$loglik)
denscomp(gp2)


gumbel2 <- fitdist(p, 'gumbel', start=list(alpha = 0, scale = 1))
gumbel2.comp <- c(gumbel2$aic, gumbel2$bic, gumbel2$loglik)

exponencial2 <- fitdist(p, 'exp')
exponencial2.comp <- c(exponencial2$aic, exponencial2$bic, exponencial2$loglik)

df <-
  data.frame(
    GEV = gev2.comp,
    GP = gp2.comp,
    Gumbel = gumbel2.comp,
    Exponencial = exponencial2.comp
  )
rownames(df) <- c('AIC','BIC', 'Log-verosimilitud')
xtable(df)

grafico4 <- denscomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), main = 'Histograma contra cada densidad', xlab = 'log-precio', ylab= 'Densidad')
grafico5 <- qqcomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), xlab = 'Cuantil teórico', ylab = 'Cuantil empírico')
grafico6 <- cdfcomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), main = 'CDF empírico y teórico', xlab = 'log-precio')
grafico7 <- ppcomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), xlab = 'Probabilidad teórica', ylab = 'Probabilidad empírica')


# pdf("graf4.pdf")
# denscomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), main = 'Histograma contra cada densidad', xlab = 'log-precio', ylab= 'Densidad')
# dev.off()
# pdf("graf5.pdf")
# qqcomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), xlab = 'Cuantil teórico', ylab = 'Cuantil empírico')
# dev.off()
# pdf("graf6.pdf")
# cdfcomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), main = 'CDF empírico y teórico', xlab = 'log-precio')
# dev.off()
# pdf("graf7.pdf")
# ppcomp(list(weibull, frechet, beta, gev2, gp2,gumbel2, exponencial2), xlab = 'Probabilidad teórica', ylab = 'Probabilidad empírica')
# dev.off()
```



Analisis diagnostico

```{r}
#se comparan los deos mejores modelos

comparacion <- gofstat(f = list(frechet, gumbel2, gev2))
comparacion

df <- tibble('Kolgomorov-Smirnoff' = comparacion$ks, 'Anderson-Darling' = comparacion$ad, 'Chi-Cuadrado'=comparacion$chisq)

rownames(df) <- c('Frechet','Gumbel','GEV')


xtable(df)
```



```{r}
descdist(p, discrete = FALSE)
```
\section{Bitácora 3
}
Estamos desarrollando un modelaje POT (Peak Over Threshold), entonces requerimos encontrar el umbral más adecuado para los datos.

Se desarrolla, entonces la función de distribucion sobre exceso de pérdida para toda la función, sin aplicar un primer umbral

```{r}
#perd <- base$logEstimado
perd <- base$Estimated_Dollar_Loss
n <- 1000 #La cantidad de puntos
d1 <- quantile(perd, 0)
d<-seq(d1,max(perd),length.out=n)

e <- function(d, X = perd){ # Funcion media de exceso para cualquier vector
  index <- lapply(d, function(e){X[X>=e]-e}  )
  sapply(index, mean)
}

p <- tibble(Umbral = d, Exceso = e(d))
# d1 <- nrow(p) - which.max(diff(rev(p$Exceso))<0) #Forma de automatizar el umbral
# d1 <- p$Umbral[d1]

grafico8 <-
  ggplot(p, aes(x = Umbral, y = Exceso)) + geom_line() + theme_bw() + 
  labs(title = "Gráfico 8: Función media de exceso",
       caption = "Fuente: Elaboración propia")
grafico8
```
Acá se puede observar claramente que la totalidad de los datos en escala regular parecen tener una media de exceso sobre umbral de tipo creciente, con lo cual utilizando lo visto en clase, una pareto generalizada puede ser ideal para el ajuste de la distribución. Ahora, si de manera arbitraria se utiliza un umbral mayor, para tratar de analizar la existencia de un comportamiento distinto en los datos, se obtiene la siguiente funcion de exceso de media.

Se nota como este comportamiento es creciente en general, si le ponemos el umbral en .95, se ve así:
```{r}
d1 <- quantile(perd, 0.95)
d<-seq(d1,max(perd),length.out=n)
p <- tibble(Umbral = d, Exceso = e(d))

grafico9 <-
  ggplot(p, aes(x = Umbral, y = Exceso)) + geom_line() + theme_bw() +
  labs(title = "Gráfico 9: Función media de exceso, iniciando el .95",
       caption = "Fuente: Elaboración propia")
grafico9
```
Dicho lo anterior, podemos escoger el umbral que maximice algún estadístico de comparación, tomamos el AIC, para cada umbral.

```{r}
d<-seq(0,quantile(perd, 0.999),length.out=100)
p <- data.frame(Umbral = d, AIC = 0)
library(mev)
library(actuar)
#gp2 <- fitdist(p, 'genpareto' , start=list(shape1 = 1, shape2=1))
for(k in 1:nrow(p)) {
  d1 <- p$Umbral[k]
  pp <- perd[perd>d1]-d1
  mod <- fit.gpd(pp, threshold = 0)
  #mod <- fit.gpd(perd, threshold = d1)
  #mod2 <-fitdist(pp, 'genpareto' , start=list(shape1 = 4, shape2=1))
  p$AIC[k] <- AIC(mod)
}


grafico10 <-
  ggplot(p, aes(x = Umbral, y = AIC)) + geom_line() + theme_bw() + 
  labs(title = "Gráfico 10: AIC en cada umbral",
       caption = "Fuente: Elaboración propia")
grafico10
```
Con lo que, el mejor modelo de ajuste de los datos es el que posea un umbral más grande, para corroborar esto se plantean tres modelos con tres umbrales distintos.

```{r}
d1<-quantile(perd, 0.85)
pp <-perd[perd>d1]-d1
mod.85 <- fit.gpd(pp, threshold = d1)

d1<-quantile(perd, 0.95)
pp <-perd[perd>d1]-d1
mod.95 <- fit.gpd(pp, threshold = d1)

d1 <- quantile(perd, 0.99)
pp <-perd[perd>d1]-d1
mod.99 <- fit.gpd(pp, threshold = d1)

df <- data.frame(AIC(mod.85), AIC(mod.95), AIC(mod.99))
rownames(df) <- 'AIC'
df
```

```{r}
Fn <- ecdf(perd)
Fx <- function(x, modelo){
  escala <- modelo$estimate[1]
  forma <- modelo$estimate[2]
  evd::pgpd(x, loc=0, scale=escala, shape=forma) 
}
m1 <- max(max(abs(Fx(perd, mod.8)-Fn(perd-.Machine$double.eps))), max(abs(Fx(perd, mod.8)-Fn(perd))))
m2 <- max(max(abs(Fx(perd, mod.95)-Fn(perd-.Machine$double.eps))), max(abs(Fx(perd, mod.95)-Fn(perd))))
m3 <- max(max(abs(Fx(perd, mod.99)-Fn(perd-.Machine$double.eps))), max(abs(Fx(perd, mod.99)-Fn(perd))))


k <- 1.22/sqrt(length(perd))
k

c(m1,m2,m2)
```
Dice kolgomorov smirnoff que ninguno sirve xd pero vale picha porque al final es escoger "el mejorcito"

\section{Modelacion opcional}
Se puede intentar modelar las pérdidas máximas esperadas al mes con los datos que se tienen, para ello se toma la media diaria de cada dato, eliminando las observaciones cuya pérdida fue de cero, pues la idea es modelar las perdidas dado que hubo alguna. Luego se selecciona el máximo observado en cada mes, y estos datos se modelan.

```{r}
library(tidyverse)
library(lubridate)
p <- tibble(Tiempo = as.Date(base$TFS_Arrival_Time), Perdida = base$Estimated_Dollar_Loss)
p <- p[p$Perdida>0,]
p <- p %>% group_by(Tiempo) %>% dplyr::summarise(Perdida = mean(Perdida))

df <- p %>%
    group_by(mes = lubridate::floor_date(Tiempo, 'month')) %>%
    dplyr::summarise(PerdidaMaxima = max(Perdida))

hist(df$PerdidaMaxima, breaks = 200, xlim = c(0,6e5))
hist(df$PerdidaMaxima, breaks = 50)
```
El gráfico está cortado para que pueda reflejar bien los datos.
```{r}
modGev <- fitdist(df$PerdidaMaxima, 'gev', start = list(loc=0, scale=1, shape=0))

modWei <- fitdist(df$PerdidaMaxima, 'weibull')

modGum <- fitdist(df$PerdidaMaxima, 'gumbel', start = list(loc=0, scale=1))

denscomp(list(modGev, modWei, modGum))
denscomp(
  list(modGev, modWei, modGum),
  fitlty = 1,
  legendtext = c("Generalizado de Valor Extremo", "Weibull", "Gumbel"),
  breaks =75,
  plotstyle = "ggplot"
) + theme_bw()

p <- tibble('Generalizado de Valor Extremo' = modGev$aic,
            'Weibull' = modWei$aic, 
            'Gumbel' = modGum$aic)
p
```
El mejor modelo para estos datos es el Weibull. Con base en ello se pueden responder preguntas cool como la probabilidad de ver una pérdida mayor a 5 millones de dólares en un mes.

```{r}
pweibull(
  5e6,
  shape = modWei$estimate[1],
  scale = modWei$estimate[2],
  lower.tail = FALSE
)
```

Esta es la mejor distribución del máximo.

En términos del objetivo general se logran modelar las pérdidas explícitas para cualquier umbral, de hecho el modelo que presenta un mejor AIC respecto a los demás es el que posee umbral cero, esto indica que el comportamiento de la función es bastante uniforme y se puede modelar como un exceso de pérdida para cualquier umbral, otra pregunta fundamental asociada al tercer objetivo específico radica en encotnrar una distribución que mejor ajuste las pérdidas, esto se logró, además de obtener una distribución del máximo para delimitar la exposición de estos eventos a desastres más graves.




\printbibliography
